# Fused Sparse C++ Kernels for Transformers

## Overview

The project implements sparse multiplication and fuses up/down projections in the MLP layers through low rank weight activations. 
Work is based on [Deja Vu](https://arxiv.org/abs/2310.17157) and Apple's [LLM in a Flash](https://arxiv.org/abs/2401.02486).

### Benefits
- **1.6-1.8x overall gain in TTFT and TPS** (4-5x gain in MLP Inference)
- **26.4%** reduction in memory usage
- **6.7Ã—** faster index selection and replacement for weight caching


```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Sparse LLM Inference Pipeline                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sparsity Selection                                              â”‚
â”‚   â”œâ”€ Hidden States â†’ LoRA Projection (Importance Scoring)       â”‚
â”‚   â”œâ”€ Binary Mask Generation: (scores > threshold)               â”‚
â”‚   â””â”€ Mask Normalization: Union across batch dimension           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Differential Weight Caching                                     â”‚
â”‚   â”œâ”€ Mask Change Detection: XOR with previous mask              â”‚
â”‚   â”œâ”€ Paired Replacement: Direct substitution algorithm          â”‚
â”‚   â””â”€ Zero-Copy Tensor Views: torch::from_blob references        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Sparse Computation                                              â”‚
â”‚   â”œâ”€ Concatenated Gate+Up Projection (Fused Operation)          â”‚
â”‚   â”œâ”€ Element-wise Activation: Ïƒ(gate) âŠ™ up                      â”‚
â”‚   â””â”€ Sparse Down Projection: Only active intermediate dims      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Keywords:** Large Language Models, Sparse Inference, Differential Weight Caching

## Performance Benchmarks
State of Implementation:
- [x] Torch CPU kernels for fp16, fp32
- [x] Differential weight caching  and selection for dynamic sparsity
- [ ] CUDA kernels for Sparse Inferencing
- [ ] CPU kernels for int8, int32, int64

### CPU Performance 
```
Sparse LLaMA 3.2 3B vs LLaMA 3.2 3B (on HuggingFace Implementation):

- Time to First Token (TTFT):  1.51Ã— faster (1.209s â†’ 0.803s)
- Output Generation Speed:     1.79Ã— faster (0.7 â†’ 1.2 tokens/sec)  
- Total Throughput:           1.78Ã— faster (0.7 â†’ 1.3 tokens/sec)
- Memory Usage:               26.4% reduction (13.25GB â†’ 9.75GB)
```

### GPU Performance

```
Sparse LLaMA 3.2 3B vs Standard LLaMA 3.2 3B CUDA Results (on HuggingFace Implementation):

- Average time (Sparse): 0.021s
- Average time (Standard): 0.018s
- CUDA Speedups: 0.86x (WIP)
```

## Usage

### Quick Benchmark

```bash
# Run comprehensive benchmark

python run_benchmark.py \
    --device cpu \                              # Device: 'cpu' or 'cuda'
    --config configs/llama_skip_causal_3b.json \ # Model configuration
    --num_runs 50 \                            # Number of benchmark runs
    --verbose True                             # Detailed timing output

# Expected output:
# âš¡ TTFT Speedup: 1.51x
# ðŸš€ Output TPS Speedup: 1.79x  
# ðŸ“Š Total Throughput Speedup: 1.78x
```

## Implementation Details

### Paired Replacement with Differential Caching
_sparse_transformers/csrc/weight_cache.h_

The weight cache is a class that manages the active weights for the sparse MLP. It differentially updates the MLP tensor memory pool for the next token based on the predicted sparsity mask.

```cpp
class WeightCache {
    // Paired replacement algorithm for differential updates
    void update_active_weights(const torch::Tensor &mask)

};
```

**Performance Impact:**
- **6.7Ã— faster cache updates**: 29.89ms (naive `index_select`) â†’ 4.46ms (paired replacement)
- **Better cache locality**: Row major for Up Projection and Column major for Down Projection Matrices
- **Contiguous Memory Access**: Single memcpy for cache updates 

### Sparse MLP Inference
_sparse_transformers/csrc/sparse_mlp_op.cpp_

```python
sparse_mlp_forward(
    x.detach(), 
    self.weight_cache.get_concat_weight(),
    self.weight_cache.get_active_down_weight(),
    self.down_proj_buffer,
    self.combined_proj_buffer,
    "silu"
)
```

**Performance Impact:**
- **5Ã— faster CPU MLP inference**: 30.1ms â†’ 6.02ms
- OpenMP parallelization with `torch::at::parallel_for`
- Bounded memory usage with weight cache memory pool

## Project Structure

```
â”œâ”€â”€ sparse_transformers/                    # C++ extension module
â”‚   â”œâ”€â”€ csrc/
â”‚   â”‚   â”œâ”€â”€ sparse_mlp_op.cpp     # Main CPU/CUDA dispatcher
â”‚   â”‚   â”œâ”€â”€ sparse_mlp_cuda.cu    # CUDA kernels
â”‚   â”‚   â””â”€â”€ weight_cache.h        # Paired replacement caching
â”‚   â”œâ”€â”€ __init__.py               # Python bindings
â”‚   â””â”€â”€ CMakeLists.txt            # Build configuration
â”œâ”€â”€ src/models/llama/
â”‚   â”œâ”€â”€ modelling_llama_skip.py   # Statistical sparsity model
â”‚   â””â”€â”€ configuration_llama_skip.py # Model configuration
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ component_timing.py       # Performance profiling
â””â”€â”€ run_benchmark.py              # End-to-end benchmarks
```

## Installation


### Build C++ Extensions
```bash
# Clone repository
git clone https://github.com/nimbleedge/sparse_transformers.git
cd sparse_transformers

# Install in editable mode (builds C++ extensions automatically)
conda create -n sparse_transformers python=3.10
conda activate sparse_transformers
conda install pytorch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 pytorch-cuda=12.4 -c pytorch -c nvidia
pip install -r requirements.txt
pip install -e .

# Verify installation
python -c "import sparse_transformers; print('âœ… Installation successful!')"
```

## Contributing
We welcome contributions from the community! Areas of particular interest are:
- **Additional models**: Extend beyond LLaMA to other architectures
- **Quantization**: Combine with INT8/FP16 optimizations
- **Attention Kernels**: Implement Sparse Attention Kernels

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

